{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*Disclaimer: Die Inhalte dieses Notebooks wurden zuletzt am 15.06.2020 geprüft. Da sich Webseiten häufig verändern, kann es sein, dass der Code zu einem späteren Zeitpunkt nicht mehr funktioniert.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Web Scraping - Die Spielregeln\n",
    "\n",
    "(*orientiert an [Pablo Barberás](http://pablobarbera.com/) Vorschlägen*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Respektiert die Wünsche der angezielten Webseite(n):\n",
    "\n",
    "    - überprüft ob eine API verfügbar ist oder die Daten anderweitig heruntergeladen werden können\n",
    "    - behaltet im Hinterkopf woher die Daten kommen, respektiert Copyrights und verweist ggf. auf die Quelle\n",
    "    - spielt mit offenen Karten, d.h. tarnt eure Programme nicht als menschliche Nutzer*innen\n",
    "\n",
    "2.  Limitiert die Serverbelastung:\n",
    "\n",
    "    -  wartet ein oder zwei Sekunden nach jeder Anfrage\n",
    "    -  scrapt nur was für euer Projekt gebraucht wird und zwar nur einmal (speichert z.B. HTML-Datein auf eurer Festplatte ab und bearbeitet diese erst im Anschluss)\n",
    "    \n",
    "3. Wie finden wir raus ob der Zugang genehmigt ist?\n",
    "\n",
    "    - einige Websites verbieten den Zugang von Scrapern über ein [`robots.txt`](http://www.robotstxt.org/robotstxt.html) file\n",
    "    - auch in den Nutzungsbedingungen (AGB's) finden sich häufig Hinweise darauf, ob Scraping erlaubt ist\n",
    "    - im Zweifelsfall immer zuerst Kontakt mit den Betreiber*innen aufnehmen\n",
    "\n",
    "\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Beispiel ResearchGate.net - ist scrapen hier erlaubt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[ResearchGate.net](https://de.wikipedia.org/wiki/ResearchGate) ist ein soziales Netzwerk für Wissenschaftler*innen.\n",
    "\n",
    "\n",
    "Erster Check: die `robots.txt` Datei: https://www.researchgate.net/robots.txt\n",
    "\n",
    "````\n",
    "User-agent: *\n",
    "Allow: /\n",
    "Disallow: /connector/\n",
    "Disallow: /plugins.\n",
    "Disallow: /firststeps.\n",
    "Disallow: /publicliterature.PublicLiterature.search.html\n",
    "Disallow: /lite.publication.PublicationRequestFulltextPromo.requestFulltext.html\n",
    "Disallow: /amp/authorize\n",
    "Allow: /signup.SignUp.html\n",
    "Disallow: /signup.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- User-agent: * bedeutet, dass die folgenden Zeilen für alle möglichen User-Agents (z.B. Google Bots, oder unsere Python Programme) gelten.\n",
    "- In der Datei ist definiert welche Bereiche nicht gescrapt werden dürfen, z.B: `/connector/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Allein auf Basis der ``robots.txt`` Datei wäre es demnach in Ordnung die Jobangebote der Seite (`/jobs/`) zu scrapen:\n",
    "<img src=\"https://www.dropbox.com/s/oxijm8u7pv30n89/researchgate_jobs.PNG?dl=1\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In den [Nutzungsbedingungen von researchgate](https://www.researchgate.net/application.TermsAndConditions.html) ist allerdings klar formuliert, dass die  Betreiber*innen Web Scraping nicht gestatten (you shall not ...):\n",
    "<img src=\"https://www.dropbox.com/s/11oxi2504bu7u8l/researchgate_tos.JPG?dl=1\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Fazit**: Ihr solltet die Seite researchgate.net nicht ohne Genehmigung der Betreiber*innen scrapen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Beispiel ted.com - ist scrapen hier erlaubt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[TED](https://de.wikipedia.org/wiki/TED_(Konferenz)) - Technology, Entertainment, Design - ist ursprünglich eine Konferenz, auf der Reden nach dem Motto \"Ideas Worth Spreading\" vorgetragen werden. Der [YouTube Channel](https://www.youtube.com/user/TEDtalksDirector) von TED ist unter den weltweit meist abonnierten Kanälen und Transkripte zu den Reden sind über die TED Homepage verfügbar.\n",
    "\n",
    "\n",
    "Erster Check: die `robots.txt` Datei: https://www.ted.com/robots.txt\n",
    "\n",
    "````\n",
    "User-agent: *\n",
    "Disallow: /latest\n",
    "Disallow: /latest-talk\n",
    "Disallow: /latest-playlist\n",
    "Disallow: /people\n",
    "Disallow: /profiles\n",
    "Disallow: /conversations\n",
    "Disallow: /themes/rss\n",
    "Disallow: /discussions\n",
    "Disallow: /tpv4\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ein paar Bereiche, wie z.B. [/profiles](https://www.ted.com/profiles) dürfen nicht programmatisch abgegriffen werden. Andere, wie z.B. [/talks](https://www.ted.com/talks) dagegen schon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In den [Nutzungsbedingungen von TED](https://www.ted.com/about/our-organization/our-policies-terms/ted-com-terms-of-use) ist angeben, dass die Inhalte [Creative Commons](https://de.wikipedia.org/wiki/Creative_Commons#Lizenzen) lizensiert sind:\n",
    "<img src=\"https://www.dropbox.com/s/zasdioadj5qgu29/ted_tos.png?dl=1\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Fazit**: \n",
    " - Die TED Inhalte dürfen - abgesehen von den in der `robots.txt` gelisteten Endpunkten - für nicht kommerzielle Zwecke gescrapt werden. \n",
    " - Die Daten sollten aber nur unter bestimmten Bedingungen weitergegeben werden.\n",
    " - Grundsätzlich sollten im Zweifelsfall die Seitenbetreiber\\*innen kontaktiert oder rechtliche Beratung aufgesucht werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Übungsaufgabe 1\n",
    "\n",
    "Prüft, ob die Webseite der HU Berlin, https://www.hu-berlin.de, gescrapt werden darf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HTML -  Hypertext Markup Language\n",
    "\n",
    "[HTML](https://de.wikipedia.org/wiki/Hypertext_Markup_Language) ist eine Sprache zur Strukturierung digitaler Dokumente und besteht aus mehreren Elementen, die in einer Baumstruktur angeordnet sind. Elemente bestehen generell aus drei Teilen:\n",
    "\n",
    "```html\n",
    "<a href=\"https://www.hu-berlin.de/\">Link zur HU Berlin</a>\n",
    "```\n",
    "\n",
    "1. Anführende bzw. schließende  **Tags**.\n",
    "2. **Attribute** des Elements innerhalb der Tags\n",
    "3. Der zu strukturierende **Text** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://www.w3schools.com/js/pic_htmltree.gif\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Was wir im Browser sehen ist eine Interpretation des HTML Dokuments:\n",
    "\n",
    "\n",
    "````\n",
    "Dokument Elemente: <head>, <body>, <footer>...\n",
    "Dokument Komponenten: <title>,<h1>, <div>...\n",
    "Textstil: <b>, <i>, <strong>...\n",
    "Hyperlinks: <a>\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Neben HTML sind hauptsächlich noch CSS und Javascript relevant für Webscraping:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### CSS\n",
    "- [Cascading Style Sheets](https://de.wikipedia.org/wiki/Cascading_Style_Sheets) (CSS) beschreiben die Formatierung und z.B. Farben von HTML Komponenten (wie z.B. ``<h1>``, ``<div>``, ...)\n",
    "- CSS ist nützlich für uns, da die CSS Zeiger (Selektoren) verwendet werden können, um HTML Elemente zu finden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Javascript\n",
    "- [Javascript](https://de.wikipedia.org/wiki/JavaScript) erweitert die Funktionalität von Webseiten (z.B. Veränderung der Inhalte nach dem laden einer Webseite)\n",
    "- Javascript wird client-seitig und nicht server-seitig ausgeführt\n",
    "- Für uns ist Javascript häufig ein Problem, da client-seitig bedingte Veränderungen des HTML Dokuments nicht über konventionelle `requests` erfasst werden können "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### HTML im Chrome Browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Ruft die [HU Berlin Seite](https://www.hu-berlin.de) im Chrome Browser auf und öffnet die Developer Tools mit der Taste `F12` (alternativ Rechtsklick + Inspect):\n",
    "    <img src=\" https://developer.chrome.com/devtools/images/elements-panel.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Setzt euren Cursor über verschiedene Elemente in der Developer Console und schaut, was auf der Homepage hervorgehoben wird.\n",
    "- Im Developer Bereich könnt ihr alle Informationen zu den Elemente, z.B. `id`, `class`, usw., auslesen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/#) ist ein Python Parser Package, über das HTML und XML Strings eingelesen und als Dokumente in einer Baumstruktur repräsentiert werden können. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## Beispiel aus der Beautiful Soup Dokumentation:\n",
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ein String, der HTML Code beinhaltet, wird zunächst über die Funktion `BeautifulSoup()` eingelesen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_doc, \"html5lib\") \n",
    "# html5lib ->  parser, ggf. vorher über pip installieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Anschließend können einzelne Attribute aus der Baumstruktur abgerufen werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(soup.prettify()) ## ähnlich wie pprint, nur für html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Besonders nützlich ist, dass im kompletten HTML Dokument nach bestimmten Tags, z.B. `a` für Links, gesucht werden kann:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(soup.find_all('a'))\n",
    "\n",
    "first_link = soup.find_all('a')[0]\n",
    "first_link.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Weiterhin können Elemente über Attribute wie `id`, `href`, oder `class` gefunden werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('id',  soup.find(id = \"link2\"))\n",
    "print('href', soup.find(href = 'http://example.com/lacie'))\n",
    "print('class', soup.find(class_ = 'story'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In sämtlichen Suchen können außerdem reguläre Ausdrücke  verwendet werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "soup.find_all('a', href = re.compile('til'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "soup.find_all('a', id = re.compile('link[\\d]+'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  \n",
    "## Web Scraping - How To\n",
    "\n",
    "1.  Struktur der Webseite verinnerlichen\n",
    "2.  Scraping Strategie wählen\n",
    "3.  Prototypen Code schreiben: Daten extrahieren, verarbeiten, validieren\n",
    "4.  Generalisieren: Funktionen, Schleifen, Debugging\n",
    "5.  Datenaufbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Der erste Versuch - HU Berlin Homepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "html = requests.get('https://www.hu-berlin.de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Request header:\\n', html.request.headers)\n",
    "print('Status Code:\\n', html.status_code)\n",
    "print('HTML String:\\n',html.text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "HTML parsen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html.text, \"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Links finden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "links = soup.find_all('a')\n",
    "links[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "links[0].get('href') # attribut 'href' des ersten soup elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "links[0].text # attribut 'text' des ersten soup elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Übungsaufgabe 2\n",
    "\n",
    "Speichert alle Links der HU-Berlin, die auf externe Webseiten verweisen, in einer Liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Code für Übungsaufgabe 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CSS Selektoren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Neben Attributen wie Klassen oder id's  ist es auch möglich Elemente über CSS Selektoren auszuwählen.\n",
    "- Der komplette Selektor Pfad kann dabei z.B. in Chrome über *Rechtsklick auf Element -> Copy -> Copy Selector* ausfindig gemacht werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "soup.select('#content a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Eine sehr nützliche Alternative (oder Ergänzung) ist das Tool [selectorgadget](http://selectorgadget.com/):\n",
    "        <img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/03/26153207/WS3.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "soup.select('.news-listing .title a') # gefunden mit selector gadget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exkurs: Fehlermeldungen abfangen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webscraping ist anfällig für unvorhergesehene Fehler. Beispiel: Ein Server reagiert nicht auf eine Anfrage. Für diese Zwecke ist es nützlich, Fehlermeldungen abzufangen um einen Programmabsturz zu vermeiden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "l = ['a', 'b', 1, 'c']\n",
    "for i in l:\n",
    "    print(i.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for i in l:\n",
    "    try:\n",
    "        print(i.upper())\n",
    "    except Exception as e: # fehler abfangen\n",
    "        print('Error for ', i, ': ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Übung 3\n",
    "\n",
    "- Macht euch mit der Struktur des [Nachrichtenarchivs](https://www.hu-berlin.de/de/pr/nachrichten/hu_foldernr_archiv) der HU Berlin Seite vertraut.\n",
    "- Verwendet das selector gadget oder eine andere Strategie um den CSS Selektor für die Links zu allen Monatsarchiven zu finden.\n",
    "- Scrapt das Nachrichtenarchiv und speichert die Links zu allen Monatsarchiven in einer Liste.\n",
    "- Geht eine Ebene tiefer und extrahiert die Links der einzelnen News Artikeln aus allen Monatsarchiven. Achtet hierbei darauf, den HU-Server nicht zu überlasten und baut [kurze Pausen](https://stackoverflow.com/questions/510348/how-can-i-make-a-time-delay-in-python) (2 Sekunden) in euer Programm ein.\n",
    "- Überprüft anschließend durch manuelle Inspektion einiger Archive, ob irgendwelche Artikellinks durch euren Code nicht erfasst wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Code für Übungsaufgabe 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scraping - Tabellarische Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In den meisten Fällen sind tabellarische Daten relativ einfach zu scrapen, da die Tabellenstruktur mit Hilfe der pandas Funktion `read_html()` direkt in einen Datensatz überführt werden kann. \n",
    "- Die Funktion erwartet als Input entweder eine URL oder einen HTML String.\n",
    "- Als Output wird eine Liste mit Dataframes aller gefundenen Tabellen generiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"https://de.wikipedia.org/wiki/Datenbanktabelle\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x19be7046688>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://de.wikipedia.org/wiki/Datenbanktabelle', width = 800, height = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Auf der Wikipedia Seite wird eine Tabelle gefunden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tables = pd.read_html('https://de.wikipedia.org/wiki/Datenbanktabelle',\n",
    "                      header = 0, # erste Zeile als Spaltenbezeichnungen\n",
    "                     flavor = 'html5lib') # parser bestimmen\n",
    "print(len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "type(tables) # Achtung: Tabellen sind in Liste abgelegt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Häufig enthält eine Seite jedoch mehrere Tabellen, so dass die gesuchte Tabelle über \"Trial and Error\" gefunden werden muss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shanghai rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "IFrame('http://www.shanghairanking.com/ARWU2008.html', width = 800, height = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "seed = 'http://www.shanghairanking.com/'\n",
    "response = requests.get(seed)\n",
    "html = response.text\n",
    "seedsoup = BeautifulSoup(html, \"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Einschub -  HTML in Files abspeichern\n",
    "\n",
    "- Um HTML Code in Files abzuspeichern können die I/O Tools von Python verwendet werden\n",
    "- Es wird lediglich das \"rohe\" HTML ohne zusätzliche Inhalte wie Bilder gespeichert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%cd \"D:\\datascraping\\data\"\n",
    "\n",
    "with open('shanghai_rankings.html', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "seedsoup.find_all('a')[:15] # alle links finden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Die Links zu allen Tabellen über eine Regular Expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "arwu_links = seedsoup.find_all('a', href = re.compile('^ARWU[0-9]{4}'))\n",
    "arwu_links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Die Links werden mit der Base URL kombiniert: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "url_list = set([seed + link.get('href') for link in arwu_links])\n",
    "sorted(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alternativ über String Formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "base = 'http://www.shanghairanking.com/ARWU{}.html' # {} = platzhalter\n",
    "for year in range(2003, 2018):\n",
    "    print(base.format(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Übungsaufgabe 4\n",
    "\n",
    "- Versucht ```robots.txt``` Datei der Webseite zu überprüfen. Was fällt auf?\n",
    "- Speichert jede Shanghai Ranking Tabelle von 2010 bis 2015 lokal in einem `.html` File ab.\n",
    "- Überführt die Tabellen anschließend in einen gemeinsamen Pandas Datensatz (siehe [pd.concat](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html)). \n",
    "- Was fällt beim betrachten der Daten auf? Welche Cleaning Prozeduren müssten an dieser Stelle noch durchgeführt werden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Code für Übungsaufgabe 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Scraping - Blogarchive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "IFrame('http://blogfraktion.de/archiv/', width = 800, height = 400) # CDU/CSU Blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach einer Inspektion der Nutzungsbedingungen sowie der ``robots.txt`` Datei (Stand: 15.06.2020) scheint Scraping in Ordnung zu sein. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gesamtarchiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "seed = 'http://blogfraktion.de/archiv/'\n",
    "response = requests.get(seed)\n",
    "seedsoup = BeautifulSoup(response.text, 'html5lib')\n",
    "seedsoup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "all_links = seedsoup.find_all('a') # alle links im archiv\n",
    "print(len(all_links))\n",
    "all_links[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Die Links zu Monatsarchiven folgen einer einheitlichen Struktur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "archive = [link.get('href') for link in seedsoup.find_all('a', \n",
    "                        href = re.compile('\\d{4}/[0-9]{2}/$'))] # regex\n",
    "sorted(archive)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Monatsarchive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "IFrame('https://blogfraktion.de/2017/05/', width = 800, height = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(archive[0])\n",
    "r = requests.get(archive[0])\n",
    "soup = BeautifulSoup(r.text, 'html5lib')\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Links zu Artikeln finden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "links = soup.find_all('a', attrs = {'rel': 'bookmark'}) # alternative: css selector\n",
    "for link in links:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Einzelne Artikel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "IFrame('https://blogfraktion.de/2019/05/06/die-woche-fuer-das-leben-im-zeichen-der-suizidpraevention/', \n",
    "width = 800, height = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "raw = requests.get('https://blogfraktion.de/2019/05/06/die-woche-fuer-das-leben-im-zeichen-der-suizidpraevention/').text\n",
    "article = BeautifulSoup(raw, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Der eigentliche Text der Blog-Artikel kann über ein Klassen-Attribut gefunden werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "main = article.find('div', { \"class\" : \"entry-content\" }).text\n",
    "print(main[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Übungsaufgabe 5\n",
    "\n",
    "- Findet einen Weg euch sämtliche Blogposts aus allen Monatsarchiven der \"blogfraktion\" zu scrapen. Achtet hierbei darauf den  Server nicht zu überlasten.\n",
    "- Erstellt aus den Posts einen Pandas Datensatz, in dem mindestens folgende Informationen für jeden Blogpost enthalten sind:\n",
    "    - Autor\n",
    "    - Datum\n",
    "    - Titel\n",
    "    - Text\n",
    "    - URL zum Post\n",
    "- Speichert den Datensatz anschließend in einem Format eurer Wahl ab.\n",
    "\n",
    "*Hinweis: Es empfiehlt sich, den Prozess in mehrere Teilschritte und entsprechende Hilfsfunktionen aufzuteilen (z.B. eine Funktion für die Extraktion von Archivlinks, eine Funktion für die Verarbeitung dieser Links und eine Funktion zur Verarbeitung von Artikellinks)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Code Übungsaufgabe 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "                \n",
    "**Kontakt: Carsten Schwemmer** (Webseite: www.carstenschwemmer.com,  Email: c.schwem2er@gmail.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
